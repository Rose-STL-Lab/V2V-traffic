{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y4xpMJZVaj_0"
   },
   "source": [
    "# V2V Traffic Forecasting - VelLSTM-FC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gy6C3DVgaj_1"
   },
   "source": [
    "Here we will torch.cat(ego, lead[:,300:], lead_l, res) i.e. input is 6000 samples by K*4 where K is the input length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMY7sx9Faj_2"
   },
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "6KBX5yecaj_3"
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat, savemat\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfQyLmD4aj_9"
   },
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "anTHrj6uaj_9"
   },
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    '''\n",
    "    Description: Convert file into ego and lead distance, time, velocity\n",
    "    Parameters: file name(input string)\n",
    "    '''\n",
    "    x = loadmat(file)\n",
    "    s = x['s']\n",
    "    t = x['t']\n",
    "    v = x['v']\n",
    "    ego_s = (s[:,0])[0]\n",
    "    ego_t = (t[:,0])[0]\n",
    "    ego_v = (v[:,0])[0]\n",
    "    lead_s = (s[:,1])[0]\n",
    "    lead_t = (t[:,1])[0] \n",
    "    lead_v = (v[:,1])[0]\n",
    "    ego_s = ego_s.ravel()\n",
    "    lead_s = lead_s.ravel()\n",
    "    ego_t = ego_t.ravel()\n",
    "    lead_t = lead_t.ravel()\n",
    "    ego_v = ego_v.ravel()\n",
    "    lead_v = lead_v.ravel()\n",
    "    return ego_s, lead_s, ego_t, lead_t, ego_v, lead_v\n",
    "\n",
    "def interpolate(e_s, l_s, e_t, l_t, e_v, l_v, size):\n",
    "    '''\n",
    "    Description: Interpolate data to fit 'size' time steps\n",
    "    Parameters: outputs of get_data, and 'size' number of time steps\n",
    "    '''\n",
    "    from scipy import interpolate\n",
    "    step_size  = (e_t[-1]-e_t[0])/size\n",
    "    f = interpolate.interp1d(e_t, e_s)\n",
    "    ego_t = np.arange(e_t[0],e_t[-1],step_size)\n",
    "    ego_s = f(ego_t)\n",
    "    f2 = interpolate.interp1d(l_t,l_s)\n",
    "    lead_t = np.arange(l_t[0],l_t[-1],step_size)\n",
    "    lead_s = f2(lead_t)\n",
    "    f3 = interpolate.interp1d(e_t, e_v)\n",
    "    ego_v = f3(ego_t)\n",
    "    f4 = interpolate.interp1d(l_t,l_v)\n",
    "    lead_v = f4(lead_t)\n",
    "    return ego_s, lead_s, ego_t, lead_t, ego_v, lead_v\n",
    "\n",
    "def rolling_window(window_size, time_start, ego_v, lead_v, ego_t, ego_s, lead_s):\n",
    "    '''\n",
    "    Description: Create rolling windows samples\n",
    "    Parameters: size of window, time step starting point, ego and lead velocity\n",
    "    '''\n",
    "    #Initialize Empty Arrays\n",
    "    inputs = np.zeros((num_samples,K))                            # inputs to the network\n",
    "    labels = np.zeros((num_samples, future_len))                  # labels to the network\n",
    "    ego_list = np.zeros((num_samples,K))                          # ego velocity from D-K to D\n",
    "    lead_list = np.zeros((num_samples,K))                         # lead velocity from B-K to B\n",
    "    ego_list_label = np.zeros((num_samples, future_len))          # ego labels from D to F\n",
    "    lead_list_label = np.zeros((num_samples, future_len))         # lead labels from B to E\n",
    "    combined_input = np.zeros((num_samples, K+future_len))        # lead labels from B to E + ego velocity from D-K to D\n",
    "    \n",
    "    for counter, time_start in enumerate(tqdm(range(time_start,time_start+num_samples))):\n",
    "        time_step_end = time_start + window_size\n",
    "        wave_y = -5*(np.arange(0,np.round(ego_t[time_step_end],decimals=1),0.1)-ego_t[time_step_end]) + ego_s[time_step_end]\n",
    "        #Intercept points refer to the points illustrated above\n",
    "        B = np.argmin(np.abs(lead_s[0:time_step_end] - wave_y))\n",
    "        D = np.argmin(np.abs(ego_s[0:time_step_end] - wave_y)) # time step end\n",
    "        E = B + future_len\n",
    "        F = D + future_len\n",
    "        ego_list[counter] = ego_v[D-K:D]\n",
    "        lead_list[counter] = lead_v[B-K:B]\n",
    "        ego_list_label[counter] = ego_v[D:F]\n",
    "        lead_list_label[counter] = lead_v[B:E]\n",
    "        inputs[counter] = lead_v[B-K:B] - ego_v[D-K:D]\n",
    "        labels[counter] = lead_v[B:E] - ego_v[D:F]\n",
    "        combined_input[counter] = np.append(lead_list_label[counter], ego_list[counter])\n",
    "    #Convert to Tensor\n",
    "    ego_list = torch.Tensor(ego_list)\n",
    "    lead_list = torch.Tensor(lead_list)\n",
    "    ego_list_label = torch.Tensor(ego_list_label)\n",
    "    lead_list_label = torch.Tensor(lead_list_label)\n",
    "    inputs = torch.Tensor(inputs)\n",
    "    labels = torch.Tensor(labels)\n",
    "    combined_input = torch.Tensor(combined_input)\n",
    "    return ego_list, lead_list, ego_list_label, lead_list_label, inputs, labels, combined_input\n",
    "\n",
    "def normalization(inputs, labels):\n",
    "    '''\n",
    "    Description: Normalize the inputs and labels\n",
    "    Parameters: inputs and the labels to network\n",
    "    '''\n",
    "    input_mean = torch.mean(inputs,1,True) \n",
    "    std = torch.std(inputs)     \n",
    "    inputs_norm = (inputs - input_mean)/std           \n",
    "    labels_norm = (labels - input_mean)/std\n",
    "    return inputs_norm, labels_norm, input_mean, std\n",
    "\n",
    "def train_val_test_split(K, future_len, combined, train_size, val_size, test_size, c_size):\n",
    "    '''\n",
    "    Description: Split data into train, test, val\n",
    "    Parameters: sizes of each section, K, future output, and the combined matrix\n",
    "    '''\n",
    "    train_matrix = torch.zeros((train_size,c_size))\n",
    "    val_matrix = torch.zeros((val_size,c_size))\n",
    "    test_matrix = torch.zeros((test_size,c_size))\n",
    "    train_matrix[:3900] = combined[:3900]\n",
    "    train_matrix[3900:] = combined[7500:11400]\n",
    "    train_matrix = train_matrix[torch.randperm(train_matrix.size()[0])]\n",
    "    val_matrix[:1300] = combined[4400:5700]\n",
    "    val_matrix[1300:2600] = combined[11900:13200]\n",
    "    val_matrix = val_matrix[torch.randperm(val_matrix.size()[0])]\n",
    "    test_matrix[:1300] = combined[6200:7500]\n",
    "    test_matrix[1300:2600] = combined[13700:15000]\n",
    "    \n",
    "    ego_train = train_matrix[:,:K]\n",
    "    ego_trainl = train_matrix[:,2*K:2*K + future_len]\n",
    "    ego_val = val_matrix[:,:K]\n",
    "    ego_vall = val_matrix[:,2*K:2*K + future_len]\n",
    "    ego_test = test_matrix[:,:K]\n",
    "    ego_testl = test_matrix[:,2*K:2*K + future_len]\n",
    "    lead_train = train_matrix[:,K:2*K]\n",
    "    lead_trainl = train_matrix[:,2*K + future_len:2*K + 2*future_len]\n",
    "    lead_val = val_matrix[:,K:2*K]\n",
    "    lead_vall = val_matrix[:,2*K + future_len:2*K + 2*future_len]\n",
    "    lead_test = test_matrix[:,K:2*K]\n",
    "    lead_testl = test_matrix[:,2*K + future_len:2*K + 2*future_len]\n",
    "    res_train = train_matrix[:,2*K + 2*future_len:3*K + 3*future_len]\n",
    "    res_trainl = train_matrix[:, 3*K + 3*future_len: 3*K + 4*future_len]\n",
    "    res_val = val_matrix[:,2*K + 2*future_len:3*K + 3*future_len]\n",
    "    res_vall = val_matrix[:, 3*K + 3*future_len: 3*K + 4*future_len]\n",
    "    res_test = test_matrix[:,2*K + 2*future_len:3*K + 3*future_len]\n",
    "    res_testl = test_matrix[:, 3*K + 3*future_len: 3*K + 4*future_len]\n",
    "    res_mean_train = train_matrix[:,-1]\n",
    "    res_mean_val = val_matrix[:,-1]\n",
    "    res_mean_test = test_matrix[:,-1]\n",
    "    res_mean = torch.unsqueeze(torch.cat([res_mean_train,res_mean_val,res_mean_test],0),1)\n",
    "    \n",
    "    return ego_train,ego_trainl,ego_val,ego_vall,ego_test,ego_testl,lead_train,lead_trainl,lead_val,lead_vall,lead_test,lead_testl,res_train,res_trainl,res_val,res_vall,res_test,res_testl, res_mean\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFCW2Pt6akAC"
   },
   "source": [
    "#### Set Up Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pl_s1Mh5akAD",
    "outputId": "affc437b-e451-43bf-932f-123cb131f6a4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7500/7500 [00:00<00:00, 9392.99it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 7500/7500 [00:01<00:00, 6613.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13000, 1])\n"
     ]
    }
   ],
   "source": [
    "interpol_size = 10000\n",
    "num_samples = 7500\n",
    "window_size = 1000\n",
    "time_start = 500\n",
    "K = 600\n",
    "future_len = window_size - K\n",
    "train_size = 3900\n",
    "val_size = 1300\n",
    "test_size = 1300\n",
    "c_size = 3*(K+future_len) + future_len + 1\n",
    "\n",
    "# Load Data\n",
    "e_s_1, l_s_1, e_t_1, l_t_1, e_v_1, l_v_1 = get_data('Dataset1.mat')\n",
    "e_s_5, l_s_5, e_t_5, l_t_5, e_v_5, l_v_5 = get_data('Dataset3.mat')\n",
    "# Interpolate\n",
    "ego_s_1, lead_s_1, ego_t_1, lead_t_1, ego_v_1, lead_v_1 = interpolate(e_s_1, l_s_1, e_t_1, l_t_1, e_v_1, l_v_1, interpol_size)\n",
    "ego_s_5, lead_s_5, ego_t_5, lead_t_5, ego_v_5, lead_v_5 = interpolate(e_s_5, l_s_5, e_t_5, l_t_5, e_v_5, l_v_5, interpol_size)\n",
    "# Rolling Window\n",
    "ego_1, lead_1, ego_l_1, lead_l_1, res_1, res_l_1, velocity_input_1 = rolling_window(window_size, time_start, ego_v_1, lead_v_1, ego_t_1, ego_s_1, lead_s_1)\n",
    "ego_5, lead_5, ego_l_5, lead_l_5, res_5, res_l_5, velocity_input_5 = rolling_window(window_size, time_start, ego_v_5, lead_v_5, ego_t_5, ego_s_5, lead_s_5)\n",
    "ego = torch.cat((ego_1, ego_5))\n",
    "lead = torch.cat((lead_1, lead_5))\n",
    "ego_l = torch.cat((ego_l_1, ego_l_5))\n",
    "lead_l = torch.cat((lead_l_1, lead_l_5))\n",
    "res = torch.cat((res_1,res_5))\n",
    "res_l = torch.cat((res_l_1, res_l_5))\n",
    "velocity_input = torch.cat((velocity_input_1, velocity_input_5))\n",
    "\n",
    "# Normalize\n",
    "input_norm, labels_norm, means, std = normalization(velocity_input, ego_l)\n",
    "# Create Combined Matrix\n",
    "combined = torch.cat([ego, lead, ego_l, lead_l, input_norm, labels_norm, means], 1)\n",
    "# Train, Val, Test Split\n",
    "ego_train,ego_trainl,ego_val,ego_vall,ego_test,ego_testl,lead_train,lead_trainl,lead_val,lead_vall,lead_test,lead_testl,res_train,res_trainl,res_val,res_vall,res_test,res_testl, res_mean = train_val_test_split(K, future_len, combined, train_size*2, val_size*2, test_size*2, c_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWA_NWklakAI"
   },
   "source": [
    "#### GPU Availability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aVjDVMnHakAJ",
    "outputId": "2531b52b-52af-4494-be35-ef8b80164121"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4Is5B2EakAO"
   },
   "source": [
    "#### Network Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "zpYGcjEcakAO"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size=1, hidden_layer_size = 200, output_size = 400):\n",
    "        super().__init__()\n",
    "        self.hidden_layer_size = hidden_layer_size\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, num_layers = 2)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "        self.hidden_cell = (torch.rand(2,1,self.hidden_layer_size),\n",
    "                            torch.rand(2,1,self.hidden_layer_size))\n",
    "    \n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, self.hidden_cell = self.lstm(input_seq.view(len(input_seq) ,1, -1), self.hidden_cell)\n",
    "        predictions = self.linear(lstm_out.view(len(input_seq), -1))\n",
    "        return predictions[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ro-NAuBvakAT"
   },
   "source": [
    "#### Creating Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kJtp6DzIakAT",
    "outputId": "0ec6f219-f9b8-4269-f1f3-a7d73dd6e3f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Module.parameters of LSTM(\n",
      "  (lstm): LSTM(1, 200, num_layers=2)\n",
      "  (linear): Linear(in_features=200, out_features=400, bias=True)\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "model = LSTM()\n",
    "# model = torch.load('600K_200H_400_AVE_data1_3_velocity_new')\n",
    "model.to(device)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005) \n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "JoC2F1CGakAX"
   },
   "outputs": [],
   "source": [
    "offical = []\n",
    "epoch_loss_list = []\n",
    "epoch_val_loss_list = []\n",
    "best_val_loss = 1.0e6\n",
    "best_AVE_val = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "id": "plcx6Zo2akAf",
    "outputId": "16296df1-5265-4e99-8cc6-15714bf50cef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                 | 2/7800 [00:00<09:39, 13.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:26<00:00, 17.46it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:08, 18.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:18<00:00, 17.80it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:59, 16.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:23<00:00, 17.59it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:01, 18.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:55<00:00, 16.40it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:05, 18.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:14<00:00, 17.95it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<06:56, 18.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:36<00:00, 17.08it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:15, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:46<00:00, 16.71it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<10:02, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:05<00:00, 16.08it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:15, 17.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:30<00:00, 17.30it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:11, 18.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:14<00:00, 15.79it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:30, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:36<00:00, 15.11it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:43, 14.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:59<00:00, 14.46it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:08, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:01<00:00, 16.19it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:53, 16.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:22<00:00, 15.53it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:00, 16.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:27<00:00, 15.37it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:26, 17.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:53<00:00, 16.47it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:15, 15.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:49<00:00, 14.72it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:09, 15.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:35<00:00, 17.11it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:29, 17.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [09:08<00:00, 14.21it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<09:08, 14.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [09:45<00:00, 13.33it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<09:03, 14.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [10:09<00:00, 12.79it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<10:25, 12.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [10:07<00:00, 12.83it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:16, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:46<00:00, 14.82it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:08, 18.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:28<00:00, 17.41it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:49, 14.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [08:27<00:00, 15.38it/s]\n",
      "  0%|                                                                                         | 0/7800 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:14<00:00, 17.94it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<06:56, 18.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:10<00:00, 18.11it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:08, 18.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:11<00:00, 18.08it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:23, 17.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:09<00:00, 18.14it/s]\n",
      "  0%|                                                                                 | 4/7800 [00:00<06:58, 18.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:10<00:00, 18.10it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:20, 17.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:11<00:00, 18.07it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<07:09, 18.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 7800/7800 [07:25<00:00, 17.52it/s]\n",
      "  0%|                                                                                 | 2/7800 [00:00<08:03, 16.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████████████████████████████████▉                                             | 3293/7800 [04:13<06:25, 11.68it/s]"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epochs = 50\n",
    "for i in range(epochs):\n",
    "    print('epoch', i)\n",
    "    epoch_loss = 0\n",
    "    epoch_val_loss = 0\n",
    "    for counter, seq in enumerate(tqdm(res_train)):\n",
    "        seq = seq.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        model.hidden_cell = (torch.rand(2, 1, model.hidden_layer_size).to(device),\n",
    "                        torch.rand(2, 1, model.hidden_layer_size).to(device))\n",
    "        y_pred = model(seq)\n",
    "        label = res_trainl[counter,0:future_len].to(device)\n",
    "        single_loss = loss_function(y_pred, label)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += single_loss.item()     \n",
    "    epoch_loss_list.append(epoch_loss)\n",
    "    with torch.no_grad():\n",
    "        prediction_val = torch.zeros((len(res_val),future_len)).to(device)   \n",
    "        for count, sample in enumerate(res_val):\n",
    "            sample = sample.to(device)\n",
    "            model.hidden_cell = (torch.rand(2, 1, model.hidden_layer_size).to(device),\n",
    "                        torch.rand(2, 1, model.hidden_layer_size).to(device))\n",
    "            val_pred = model(sample)\n",
    "            val_label = res_vall[count,0:future_len].to(device)\n",
    "            val_single_loss = loss_function(val_pred,val_label)\n",
    "            epoch_val_loss += val_single_loss.item()\n",
    "            prediction_val[count] = model(sample)\n",
    "    \n",
    "    prediction_val = prediction_val.cpu().detach().numpy()   \n",
    "    unnorm_pred_val = (prediction_val*std.cpu().detach().numpy()) + res_mean[-test_size*2:].numpy()     \n",
    "    final_real_val = torch.Tensor(unnorm_pred_val)\n",
    "    AVE_val = np.mean(np.abs(np.array(ego_testl)-np.array(final_real_val)))\n",
    "    if AVE_val < best_AVE_val:\n",
    "        torch.save(model, \"600K_200H_400_AVE_data1_3_velocity_table1_run5\")\n",
    "        best_AVE_val = AVE_val\n",
    "\n",
    "    epoch_val_loss_list.append(epoch_val_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EzOFsF1akAi"
   },
   "source": [
    "#### Plot Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdL7ObqOakAj",
    "outputId": "d852d9a9-79a4-4722-990f-06264a377b77"
   },
   "outputs": [],
   "source": [
    "plt.plot(epoch_loss_list, label = 'Training Loss')\n",
    "plt.plot(epoch_val_loss_list, label = 'Validation Loss')\n",
    "plt.legend()\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('600K_200H_400_AVE_data1_3_velocity_table1_run5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xD8MkNm2akAn"
   },
   "source": [
    "#### Generate Testing Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9XAOttLakAn"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    prediction = torch.zeros((len(res_test),future_len)).to(device)   \n",
    "    for i, seq in enumerate(res_test):\n",
    "        seq = seq.to(device)\n",
    "        model.hidden = (torch.rand(2, 1, model.hidden_layer_size).to(device),\n",
    "                            torch.rand(2, 1, model.hidden_layer_size).to(device))\n",
    "        prediction[i] = model(seq)\n",
    "prediction = prediction.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ADBQAgOakAr"
   },
   "source": [
    "#### Visualize One Sample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rma4povKakAr",
    "outputId": "a8d0a3e8-7bd6-4bfa-f505-139d5e47c144"
   },
   "outputs": [],
   "source": [
    "#Choose Sample\n",
    "sample = 100\n",
    "\n",
    "#Visualize Prediction Residual with Input\n",
    "plt.plot(res_test[sample], label = \"Input\")\n",
    "plt.plot(list(range(K,K+future_len)),res_testl[sample], label = \"Truth\")\n",
    "plt.plot(list(range(K,K+future_len)),prediction[sample], label = \"Prediction\")\n",
    "plt.xlabel(\"time steps\")\n",
    "plt.ylabel(\"normalized residual\")\n",
    "plt.title(\"Visualize prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mU7LSYH2akAv"
   },
   "source": [
    "#### Unnormalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aWfv7A81akAw",
    "outputId": "e160813e-d4fb-45f2-832d-b512d7172464"
   },
   "outputs": [],
   "source": [
    "unnorm_pred = (prediction*std.cpu().detach().numpy()) + res_mean[-test_size*2:].numpy()\n",
    "unnorm_label = (res_testl.cpu().detach().numpy()*std.cpu().detach().numpy()) + res_mean[-test_size*2:].numpy()\n",
    "unnorm_input = (res_test.cpu().detach().numpy()*std.cpu().detach().numpy()) + res_mean[-test_size*2:].numpy()\n",
    "#Plot Unnormalized Residual\n",
    "for sample in range(100):\n",
    "    plt.plot(unnorm_input[sample], label = \"Input\")\n",
    "    plt.plot(list(range(K,K+future_len)), unnorm_label[sample], label = \"Truth\")\n",
    "    plt.plot(list(range(K,K+future_len)),unnorm_pred[sample], label = 'Prediction')\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Residual Velocity (m/s)\")\n",
    "    plt.title(\"Prediction and Truth\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGQ3AjMWakA0"
   },
   "source": [
    "#### Final Realization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7CGRSxnakA1",
    "outputId": "e3bcbe0a-5483-4a10-b233-8d57e76a176a"
   },
   "outputs": [],
   "source": [
    "#Define the final realization\n",
    "final_real_test = torch.Tensor(unnorm_pred)\n",
    "constant_V = torch.ones(test_size*2, future_len)\n",
    "for i in range(test_size*2):\n",
    "    constant_V[i] = ego_test[i,-1]\n",
    "\n",
    "#Prepare to plot the final realization\n",
    "for sample in range(1000):\n",
    "    print(sample)\n",
    "    plt.plot(np.array(range(K)) / 10, ego_test[sample], label = 'Ego Velocity (Input)')\n",
    "    plt.plot(np.array(range(K)) / 10, lead_test[sample], label = 'Translated Lead Velocity')\n",
    "    plt.plot(np.array(range(K,K+future_len)) / 10, constant_V[sample], label = 'Constant Velocity')\n",
    "    plt.plot(np.array(range(K,K+future_len)) / 10,ego_testl[sample], label = 'Ego Truth' )\n",
    "    plt.plot(np.array(range(K,K+future_len)) / 10,lead_testl[sample], label = 'Translated Lead Truth')\n",
    "    plt.plot(np.array(range(K,K+future_len)) / 10, final_real_test[sample] , label = 'Prediction')  \n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Velocity (m/s)')\n",
    "    plt.title('Final Realization')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k_TwZdzHakA5",
    "outputId": "865d3416-b0d9-4ebb-92d4-29fd3efa095b"
   },
   "outputs": [],
   "source": [
    "sample = 9\n",
    "AVE_sample = np.mean(np.abs(np.array(ego_testl[sample,:]) - np.array(final_real_test[sample,:])))\n",
    "AVE_trans = np.mean(np.abs(np.array(ego_testl[sample,:]) - np.array(lead_testl[sample,:])))\n",
    "AVE_const = np.mean(np.abs(np.array(ego_testl[sample,:]) - np.array(constant_V[sample,:])))\n",
    "print(AVE_sample)\n",
    "print(AVE_trans)\n",
    "print(AVE_const)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a8gfPYHSakA8"
   },
   "source": [
    "#### Evaluation Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yR-pUmnwakA9",
    "outputId": "a482731c-0690-4fbc-8e22-8cd9605aa6ee"
   },
   "outputs": [],
   "source": [
    "# Average Velocity Error Calculation\n",
    "ts = 99 # time step at which we want to calculate VE\n",
    "AVE_ml = np.mean(np.abs(np.array(ego_testl)-np.array(final_real_test)))\n",
    "AVE_trans = np.mean(np.abs(np.array(ego_testl) - np.array(lead_testl)))\n",
    "AVE_const = np.mean(np.abs(np.array(ego_testl) - np.array(constant_V)))\n",
    "VE_const = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(constant_V[:,ts])))\n",
    "VE_trans = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(lead_testl[:,ts])))\n",
    "VE_ml = np.mean(np.abs(np.array(ego_testl[:,ts])-np.array(final_real_test[:,ts])))\n",
    "print(\"AVE : \", AVE_ml, \"m/s\")\n",
    "print(\"Translated AVE: \", AVE_trans, \"m/s\")\n",
    "print(\"Constant AVE:\" , AVE_const, \"m/s\")\n",
    "print(\"VE ml at\",ts, ':', VE_ml, \"m/s\")\n",
    "print(\"VE trans at\",ts, ':', VE_trans, \"m/s\")\n",
    "print(\"VE Constant at\",ts, ':', VE_const, \"m/s\")\n",
    "ts = 199 # time step at which we want to calculate VE\n",
    "VE_const = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(constant_V[:,ts])))\n",
    "VE_trans = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(lead_testl[:,ts])))\n",
    "VE_ml = np.mean(np.abs(np.array(ego_testl[:,ts])-np.array(final_real_test[:,ts])))\n",
    "print(\"VE ml at\",ts, ':', VE_ml, \"m/s\")\n",
    "print(\"VE trans at\",ts, ':', VE_trans, \"m/s\")\n",
    "print(\"VE Constant at\",ts, ':', VE_const, \"m/s\")\n",
    "ts = 299 # time step at which we want to calculate VE\n",
    "VE_const = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(constant_V[:,ts])))\n",
    "VE_trans = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(lead_testl[:,ts])))\n",
    "VE_ml = np.mean(np.abs(np.array(ego_testl[:,ts])-np.array(final_real_test[:,ts])))\n",
    "print(\"VE ml at\",ts, ':', VE_ml, \"m/s\")\n",
    "print(\"VE trans at\",ts, ':', VE_trans, \"m/s\")\n",
    "print(\"VE Constant at\",ts, ':', VE_const, \"m/s\")\n",
    "ts = 399 # time step at which we want to calculate VE\n",
    "VE_const = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(constant_V[:,ts])))\n",
    "VE_trans = np.mean(np.abs(np.array(ego_testl[:,ts]) - np.array(lead_testl[:,ts])))\n",
    "VE_ml = np.mean(np.abs(np.array(ego_testl[:,ts])-np.array(final_real_test[:,ts])))\n",
    "print(\"VE ml at\",ts, ':', VE_ml, \"m/s\")\n",
    "print(\"VE trans at\",ts, ':', VE_trans, \"m/s\")\n",
    "print(\"VE Constant at\",ts, ':', VE_const, \"m/s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lzo0OsWdakBA"
   },
   "source": [
    "#### Velocity Error at Time Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Cr6ACJxakBB",
    "outputId": "144ff004-608a-42c5-abe8-aee9ead113bd"
   },
   "outputs": [],
   "source": [
    "#Velocity at time step Error Calculation\n",
    "VE_test = np.zeros(future_len)\n",
    "VE_test_trans = np.zeros(future_len)\n",
    "VE_constant = np.zeros(future_len)\n",
    "time_step = list(range(0,future_len))\n",
    "\n",
    "for i in range(future_len):\n",
    "    VE_test[i] = np.mean(np.abs(np.array(ego_testl)[:,i]- np.array(final_real_test)[:,i]))\n",
    "    VE_test_trans[i] = np.mean(np.abs(np.array(ego_testl)[:,i] - np.array(lead_testl)[:,i]))\n",
    "    VE_constant[i] = np.mean(np.abs(np.array(ego_testl[:,i]) - np.array(constant_V[:,i])))\n",
    "#Prepare to plot Testing Velocity Error  \n",
    "plt.plot(np.array(time_step)/10, VE_test, label = 'VelLSTM-FC')\n",
    "plt.plot(np.array(time_step)/10, VE_constant, label = 'Constant')\n",
    "#Prepare to plot Translated Velocity Error\n",
    "plt.plot(np.array(time_step)/10, VE_test_trans, label = 'Translated')\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Velocity Error (m/s)\")\n",
    "plt.title(\"Velocity Error at time step\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Save experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvIhbnfSakBE"
   },
   "outputs": [],
   "source": [
    "mdict = {'ml_test_output':np.array(final_real_test), 'ego_test':np.array(ego_test), 'ego_truth': np.array(ego_testl), 'lead_shifted': np.array(lead_testl), 'lead_past': np.array(lead_test)}\n",
    "savemat('VelLSTM-FC_Table1_5.mat', mdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "V2V Traffic Forecasting - L4DC - 7 - Velocity - File1_5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
